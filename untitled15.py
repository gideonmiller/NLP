# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mL8QPWaevysJOnWpufCdrMpw-Q3PcAIv
"""

pip install transformers

import pandas as pd
import numpy as np

from transformers import BertTokenizer, BertForSequenceClassification, pipeline
import torch
from sklearn.metrics import precision_score, recall_score, f1_score
from typing import Dict, Union, Any

lawdata = pd.read_csv("/content/sample_data/justice.csv")
lawdata

lawdata['facts'] = lawdata['facts'].str.replace('<p>', '')
lawdata['vote'] = lawdata.apply(lambda row: row['majority_vote'] if row['first_party_winner'] else row['minority_vote'], axis=1)
lawdata['complexity'] = lawdata['majority_vote'] - lawdata['minority_vote'].abs()
lawdata['missing'] = 9 - lawdata['majority_vote'] - lawdata['minority_vote'].abs()

lawdata.dropna(subset=['first_party_winner'], inplace=True)

lawdata[['facts', 'term', 'issue_area', 'vote','complexity','missing','first_party_winner']]

lawdata['issue_area_num'], _ = pd.factorize(lawdata['issue_area'])

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#lawdata[['facts', 'term', 'issue_area_num', 'vote','complexity','missing','first_party_winner']]

train_data, test_data = train_test_split(lawdata[['facts', 'term', 'issue_area_num', 'vote','complexity','missing','first_party_winner']]
, test_size=0.2, random_state=42)

class CourtCaseDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx]['facts']
        label = self.data.iloc[idx]['first_party_winner']
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }



from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
max_length = 512

train_dataset = CourtCaseDataset(train_data, tokenizer, max_length)
test_dataset = CourtCaseDataset(test_data, tokenizer, max_length)

from transformers import DistilBertForSequenceClassification, DistilBertTokenizer


model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    learning_rate=.000005,
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    prec = precision_score(labels, preds)
    rec = recall_score(labels, preds)
    f1 = f1_score(labels, preds)
    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
#.617021 accuracy with max length 256
#Epoch	Training Loss	Validation Loss	Accuracy	Precision	Recall	F1
#1	0.658500	0.666159	0.617021	0.617021	1.000000	0.763158
#2	0.644600	0.678234	0.617021	0.617021	1.000000	0.763158
#3	0.614700	0.712488	0.588146	0.617801	0.871921	0.723187





from sklearn.preprocessing import StandardScaler

train_data['term'] = train_data['term'].apply(lambda x: int(str(x)[:4]))
test_data['term'] = test_data['term'].apply(lambda x: int(str(x)[:4]))

non_text_features_columns = ['term', 'issue_area_num', 'vote', 'complexity', 'missing']
scaler = StandardScaler()
train_data[non_text_features_columns] = scaler.fit_transform(train_data[non_text_features_columns])
test_data[non_text_features_columns] = scaler.transform(test_data[non_text_features_columns])

class CourtCaseDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx]['facts']
        label = self.data.iloc[idx]['first_party_winner']
        non_text_features_np = self.data.iloc[idx][['term', 'issue_area_num', 'vote', 'complexity', 'missing']].values.astype(np.float32)
        non_text_features = torch.tensor(non_text_features_np, dtype=torch.float)
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'non_text_features': non_text_features,
            'labels': torch.tensor(label, dtype=torch.long)
        }

train_dataset = CourtCaseDataset(train_data, tokenizer, max_length)
test_dataset = CourtCaseDataset(test_data, tokenizer, max_length)

from torch import nn
from transformers import DistilBertForSequenceClassification

class DistilBertWithNonTextFeatures(DistilBertForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.num_non_text_features = 5  # Number of non-text features
        self.dense_combined = nn.Linear(config.dim + self.num_non_text_features, config.dim)
        self.dropout_combined = nn.Dropout(config.seq_classif_dropout)

    def forward(self, input_ids, attention_mask, non_text_features, labels=None):
        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)
        pooled_output = hidden_state[:, 0]  # (bs, dim)

        combined_features = torch.cat((pooled_output, non_text_features), dim=-1)
        combined_output = self.dense_combined(combined_features)
        combined_output = nn.ReLU()(combined_output)
        combined_output = self.dropout_combined(combined_output)

        logits = self.classifier(combined_output)

        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
            return (loss, logits)
        else:
            return (logits,)
#new, previous is original
#class DistilBertWithNonTextFeatures(DistilBertForSequenceClassification):
#    def __init__(self, config, class_weights=None):
#        super().__init__(config)
#        self.num_non_text_features = 5  # Number of non-text features
#        self.dense_combined = nn.Linear(config.dim + self.num_non_text_features, config.dim)
#        self.dropout_combined = nn.Dropout(config.seq_classif_dropout)
#        self.class_weights = class_weights

#    def forward(self, input_ids, attention_mask, non_text_features, labels=None):
#        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)
#        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)
#        pooled_output = hidden_state[:, 0]  # (bs, dim)

#        combined_features = torch.cat((pooled_output, non_text_features), dim=-1)
#        combined_output = self.dense_combined(combined_features)
#        combined_output = nn.ReLU()(combined_output)
#        combined_output = self.dropout_combined(combined_output)

#        logits = self.classifier(combined_output)

#        if labels is not None:
#            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)
#            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
#            return (loss, logits)
#        else:
#            return (logits,)

model = DistilBertWithNonTextFeatures.from_pretrained('distilbert-base-uncased', num_labels=2)

#all below in chunk is new. above is original
#from sklearn.utils.class_weight import compute_class_weight
#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#class_weights = compute_class_weight('balanced', classes=np.unique(train_data['first_party_winner']), y=train_data['first_party_winner'])
#class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

#model = DistilBertWithNonTextFeatures.from_pretrained('distilbert-base-uncased', num_labels=2, class_weights=class_weights)
#model = model.to(device)

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        non_text_features = inputs.pop("non_text_features")
        input_ids = inputs.pop("input_ids")
        attention_mask = inputs.pop("attention_mask")

        outputs = model(input_ids, attention_mask, non_text_features, labels=labels)
        loss = outputs[0]

        if return_outputs:
            return loss, outputs

        return loss
class CourtCaseCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, examples):
        labels = [example["labels"] for example in examples]
        non_text_features = [example["non_text_features"] for example in examples]

        input_ids = [example["input_ids"] for example in examples]
        attention_mask = [example["attention_mask"] for example in examples]

        input_ids = torch.stack(input_ids, dim=0)
        attention_mask = torch.stack(attention_mask, dim=0)
        non_text_features = torch.stack(non_text_features, dim=0)
        labels = torch.tensor(labels, dtype=torch.long)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "non_text_features": non_text_features,
            "labels": labels,
        }

data_collator = CourtCaseCollator(tokenizer)


training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    learning_rate=.0000005,
)
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

trainer.train()
#.617

from sklearn.metrics import confusion_matrix
predictions = trainer.predict(test_dataset)
true_labels = predictions.label_ids
predicted_labels = predictions.predictions.argmax(-1)
conf_matrix = confusion_matrix(true_labels, predicted_labels)

print(conf_matrix)











non_text_features_columns = ['term', 'issue_area_num']
scaler = StandardScaler()
train_data[non_text_features_columns] = scaler.fit_transform(train_data[non_text_features_columns])
test_data[non_text_features_columns] = scaler.transform(test_data[non_text_features_columns])

class CourtCaseDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx]['facts']
        label = self.data.iloc[idx]['first_party_winner']
        non_text_features_np = self.data.iloc[idx][['complexity', 'missing']].values.astype(np.float32)
        non_text_features = torch.tensor(non_text_features_np, dtype=torch.float)
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'non_text_features': non_text_features,
            'labels': torch.tensor(label, dtype=torch.long)
        }

train_dataset = CourtCaseDataset(train_data, tokenizer, max_length)
test_dataset = CourtCaseDataset(test_data, tokenizer, max_length)
from torch import nn
from transformers import DistilBertForSequenceClassification

class DistilBertWithNonTextFeatures(DistilBertForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.num_non_text_features = 2  # Number of non-text features
        self.dense_combined = nn.Linear(config.dim + self.num_non_text_features, config.dim)
        self.dropout_combined = nn.Dropout(config.seq_classif_dropout)

    def forward(self, input_ids, attention_mask, non_text_features, labels=None):
        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)
        pooled_output = hidden_state[:, 0]  # (bs, dim)

        combined_features = torch.cat((pooled_output, non_text_features), dim=-1)
        combined_output = self.dense_combined(combined_features)
        combined_output = nn.ReLU()(combined_output)
        combined_output = self.dropout_combined(combined_output)

        logits = self.classifier(combined_output)

        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
            return (loss, logits)
        else:
            return (logits,)
model = DistilBertWithNonTextFeatures.from_pretrained('distilbert-base-uncased', num_labels=2)
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        non_text_features = inputs.pop("non_text_features")
        input_ids = inputs.pop("input_ids")
        attention_mask = inputs.pop("attention_mask")

        outputs = model(input_ids, attention_mask, non_text_features, labels=labels)
        loss = outputs[0]

        if return_outputs:
            return loss, outputs

        return loss
class CourtCaseCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, examples):
        labels = [example["labels"] for example in examples]
        non_text_features = [example["non_text_features"] for example in examples]

        input_ids = [example["input_ids"] for example in examples]
        attention_mask = [example["attention_mask"] for example in examples]

        input_ids = torch.stack(input_ids, dim=0)
        attention_mask = torch.stack(attention_mask, dim=0)
        non_text_features = torch.stack(non_text_features, dim=0)
        labels = torch.tensor(labels, dtype=torch.long)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "non_text_features": non_text_features,
            "labels": labels,
        }

data_collator = CourtCaseCollator(tokenizer)


training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    learning_rate=.0000005,
)
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

def l1_regularization_loss(model, lambda_l1=0.01):
    l1_loss = 0.0
    for param in model.parameters():
        l1_loss += torch.sum(torch.abs(param))
    return lambda_l1 * l1_loss

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    learning_rate=1e-4,
    weight_decay=0.00001,
)


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        non_text_features = inputs.pop("non_text_features")
        input_ids = inputs.pop("input_ids")
        attention_mask = inputs.pop("attention_mask")

        outputs = model(input_ids, attention_mask, non_text_features, labels=labels)
        loss = outputs[0]

        # L1 regularization is removed

        if return_outputs:
            return loss, outputs

        return loss

trainer.train()

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=6,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    learning_rate=.000005,
)

import numpy as np
from sklearn.model_selection import KFold

def get_kfold_data(data, k):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    splits = []
    
    for train_index, test_index in kf.split(data):
        train_data = data.iloc[train_index]
        test_data = data.iloc[test_index]
        splits.append((train_data, test_data))
    
    return splits
k_folds = 5
data_splits = get_kfold_data(lawdata[['facts', 'term', 'issue_area_num', 'vote','complexity','missing','first_party_winner']], k_folds)

# Modify train and test data in each split
for i in range(len(data_splits)):
    train_data, test_data = data_splits[i]
    train_data['term'] = train_data['term'].str.slice(stop=4).astype(int)
    test_data['term'] = test_data['term'].str.slice(stop=4).astype(int)
    data_splits[i] = (train_data, test_data)

for fold, (train_data, test_data) in enumerate(data_splits):
    print(f"Fold {fold + 1}/{k_folds}")
    
    # Preprocessing
    scaler = StandardScaler()

    train_data[non_text_features_columns] = scaler.fit_transform(train_data[non_text_features_columns])
    test_data[non_text_features_columns] = scaler.transform(test_data[non_text_features_columns])

    # Create train and test datasets
    train_dataset = CourtCaseDataset(train_data, tokenizer, max_length)
    test_dataset = CourtCaseDataset(test_data, tokenizer, max_length)
    
    # Instantiate the model
    model = DistilBertWithNonTextFeatures.from_pretrained('distilbert-base-uncased', num_labels=2)
    
    # Instantiate the trainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
        data_collator=data_collator,
    )
    
    # Train and evaluate the model
    trainer.train()
    
    # You can also save the model after each fold if needed
    # model.save_pretrained(f'./results/fold-{fold + 1}')